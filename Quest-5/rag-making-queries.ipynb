{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Making Queries to the RAG Model\n",
    "In this Python notebook, we will be making use of our RAG model as well as an LLM to ask questions regarding our uploaded documents. If all goes to plan, our RAG model (powered by Atlas Vector Search) should be able to retrieve the portions of the document that's relevant to our query and feed that information to the LLM, thus enabling it to correctly answer our query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup\n",
    "Same as with the earlier Python notebook we used, we'll start with some basic setup steps in the next two code cells. Don't worry if your device does not have a GPU, you will still be able to proceed with the Quest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is enabled\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# To disable GPU and experiment, uncomment the following line\n",
    "# Normally, you would want to use GPU, if one is available\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging. To see more logging, set the level to DEBUG\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# Change system path to root direcotry\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# For debugging purposes\n",
    "# print (config)\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "else:\n",
    "    print(\"ATLAS_URI Connection string found:\", ATLAS_URI)\n",
    "\n",
    "OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise Exception (\"'OPENAI_API_KEY' is not set. Please set it above to continue...\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY Connection string found:\", OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our variables\n",
    "DB_NAME = 'rag1'\n",
    "COLLECTION_NAME = '10k'\n",
    "INDEX_NAME = 'idx_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex will download embeddings models as needed\n",
    "# Set llamaindex cache dir to ../cache dir here (Default is system tmp)\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "mongodb_client = MongoClient(ATLAS_URI)\n",
    "\n",
    "print (\"Atlas client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Embedding Model\n",
    "\n",
    "Now, we'll need to set up an embedding model to help us generate embeddings for the user query. \n",
    "\n",
    "Same as in the previous Python notebook in this Quest, we'll have the option to either use OpenAI models or open source HuggingFace models. We'll be going with the second approach here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Option A: OpenAI Embeddings\n",
    "\n",
    "This option utilizes an OpenAI embedding model. As such, you will need to have an OpenAI API key (as defined in env variable `OPENAI_API_KEY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only uncomment this if you are using OpenAI for Embeddings\n",
    "# from llama_index import  OpenAIEmbedding\n",
    "# embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Option B: Using Custom Embeddings\n",
    "\n",
    "This option utilizes a HuggingFace embedding model. Note that this embedding model must be the same as the embedding model you used in the previous Python notebook when you were generating the embeddings for the documents. Unless you changed it, it should be `BAAI/bge-small-en-v1.5` in both Python notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup LLM\n",
    "Then, we'll need to setup an LLM to be able to take the results from the Atlas Vector Search and respond to the user query. We'll be using OpenAI for this purpose, so make sure that your OpenAI key in your .env file is populated.\n",
    "\n",
    "After doing that, run the code cell. We'll use a simple completion task to see if our LLM has successfully loaded before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "openai.api_key = config.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "completion_response = llm.complete(\"To infinity, and\")\n",
    "print(completion_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now that we've initialized both our embedding model as well as our LLM, let's combine them together into a unified interface `service_context` that we can use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Connect Llama-Index and MongoDB Atlas\n",
    "\n",
    "This is where everything comes together, we orchestrate the combination of MongoDB Atlas as our vector storage and the `service_context` we just defined. This system we've just set up will allow us to ask the LLM questions regarding our uploaded documents; Atlas Vector Search will then locate portions of the document that most closely matches our query to supplement the LLM's response, thereby providing us with a more accurate response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.core import StorageContext\n",
    "\n",
    "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "# Uncomment the line below and comment away the line above if you face an import error\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = 'idx_embedding',\n",
    "                                 ## the following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query Data / Ask Questions\n",
    "\n",
    "Now, time for the fun part - asking it some questions! Let's start with asking our model 2 questions where the answers can be found in our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from pprint import pprint\n",
    "\n",
    "response = index.as_query_engine().query(\"What was Uber's revenue?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "pprint(response, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.as_query_engine().query(\"How much money did Lyft make in 2020?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "pprint(response, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the 2 questions we asked above, our model was able to search for portions within the uploaded documents that most closely matched our queries and responded with the answers. Now, let's try asking it a question where the answer can't be found in the uploaded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The answer to this question doesn't exist in the Lyft_10k filing!\n",
    "# Let's see what we get back\n",
    "response = index.as_query_engine().query(\"How much money Lyft made in 2018?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "pprint(response, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The answer to this question doesn't exist in the Uber_10k filing either\n",
    "# Let's see what we get back\n",
    "response = index.as_query_engine().query(\"How many employees did Uber have in 2015?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "pprint(response, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job following till the end! Please **head back to the Quest page on StackUp now** and refer to the instructions for how you can prepare your deliverable for this Quest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
